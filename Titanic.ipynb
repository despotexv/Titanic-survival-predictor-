{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and exploration \n",
    "In the data cleaning and exploration phase, the procedure involves several crucial steps to prepare the dataset for further analysis:\n",
    "\n",
    "1.\tImporting the Data: The first step involves loading the dataset into our working environment. This is typically done using libraries such as Pandas in Python, which allows for reading data from various sources like CSV files, Excel spreadsheets, databases, etc. The goal here is to get the raw data into a format that can be easily manipulated and analyzed.\n",
    "\n",
    "2.\tExamining Summary Statistics and Histograms: Once the data is imported, the next step is to explore it by looking at summary statistics. This includes generating descriptive statistics like mean, median, standard deviation, minimum, and maximum values for numerical data. For categorical data, it might involve looking at the frequency of different categories. These statistics provide an initial understanding of the distribution and central tendencies of the data, helping identify any anomalies, patterns, or trends.\n",
    "\t\n",
    "3.\tEvaluating Null Values: The final step in this initial phase is assessing the presence of null (missing) values in the dataset. Null values can significantly impact the quality of analyses and models. This step involves identifying the columns that contain null values, understanding the proportion of these null values in each column, and deciding on a strategy for handling them. Strategies for dealing with null values include imputation (replacing nulls with substituted values like the mean or median), deletion (removing records with null values), or sometimes leaving them as-is, depending on the context and significance of the missing data.\n",
    "\n",
    "By methodically performing these steps, you ensure that the dataset is well-understood and primed for more complex analyses or modeling. This phase is crucial as it lays the groundwork for any data-driven insights or conclusions drawn from the subsequent stages of the data analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "training = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "training['train_test'] = 1\n",
    "test['train_test'] = 0\n",
    "test['Survived'] = np.NaN\n",
    "all_data = pd.concat([training,test])\n",
    "\n",
    "# Function to create a simply styled, scrollable table within a small window\n",
    "def create_scrollable_table(df, table_id, title):\n",
    "    html = f'<h3 style=\"color: #333; font-family: Arial, sans-serif;\">{title}</h3>'\n",
    "    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto; border: 1px solid #ccc; border-radius: 4px;\">'\n",
    "    html += df.to_html(border=0)\n",
    "    html += '</div>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical features\n",
    "numerical_feats = training.select_dtypes(include=[np.number])\n",
    "summary_stats = numerical_feats.describe().T\n",
    "\n",
    "# Use the previously defined function to create a styled, scrollable table\n",
    "html_numerical = create_scrollable_table(summary_stats, 'numerical_features', 'Summary Statistics for Numerical Features')\n",
    "\n",
    "# Enhanced display with HTML formatting\n",
    "display(HTML(html_numerical))\n",
    "\n",
    "# Plot histograms for specific numerical features\n",
    "selected_numericals  =  training[['Age','SibSp','Parch','Fare']]\n",
    "for feature in selected_numericals: \n",
    "    plt.hist(numerical_feats[feature], bins=15, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n",
    "\n",
    "# compare survival rate across Age, SibSp, Parch, and Fare \n",
    "pd.pivot_table(training, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for categorical features\n",
    "categorical_features = training.select_dtypes(include=[object])\n",
    "cat_summary_stats = categorical_features.describe().T\n",
    "\n",
    "# Use the previously defined function to create a styled, scrollable table\n",
    "html_categorical = create_scrollable_table(cat_summary_stats, 'categorical_features', 'Summary Statistics for Categorical Features')\n",
    "\n",
    "# Enhanced display with HTML formatting\n",
    "display(HTML(html_categorical))\n",
    "\n",
    "# Plot histograms for specific categorical features\n",
    "selected_categorical  =  training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]\n",
    "for i, column in enumerate(selected_categorical.columns, 1):\n",
    "    plt.figure(figsize=(8, 4))  # Set the size of each individual plot\n",
    "    sns.countplot(x=column, hue=column, data=selected_categorical, palette='Set2', legend=False)\n",
    "    plt.title(f'Distribution of {column}', fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks(rotation=45)  # Rotate the x labels for better readability\n",
    "    plt.show()  # Display each plot individually\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing survival with each of these categorical variables\n",
    "\n",
    "# Survival vs Passenger Class (Pclass)\n",
    "pclass_pivot = pd.pivot_table(training, index='Survived', columns='Pclass', values='Ticket', aggfunc='count')\n",
    "print(\"Survival vs Passenger Class (Pclass):\\n\", pclass_pivot, \"\\n\")\n",
    "\n",
    "# Survival vs Sex\n",
    "sex_pivot = pd.pivot_table(training, index='Survived', columns='Sex', values='Ticket', aggfunc='count')\n",
    "print(\"Survival vs Sex:\\n\", sex_pivot, \"\\n\")\n",
    "\n",
    "# Survival vs Embarked\n",
    "embarked_pivot = pd.pivot_table(training, index='Survived', columns='Embarked', values='Ticket', aggfunc='count')\n",
    "print(\"Survival vs Embarked:\\n\", embarked_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in the dataset\n",
    "null_values = training.isnull().sum()\n",
    "html_null_values = create_scrollable_table(null_values.to_frame(), 'null_values', 'Null Values in the Dataset')\n",
    "\n",
    "# Percentage of missing values for each feature\n",
    "missing_percentage = (training.isnull().sum() / len(training)) * 100\n",
    "html_missing_percentage = create_scrollable_table(missing_percentage.to_frame(), 'missing_percentage', 'Percentage of Missing Values for Each Feature')\n",
    "\n",
    "# Exploring rows with missing values\n",
    "rows_with_missing_values = training[training.isnull().any(axis=1)]\n",
    "html_rows_with_missing_values = create_scrollable_table(rows_with_missing_values.head(), 'rows_with_missing_values', 'Rows with Missing Values')\n",
    "\n",
    "# Display both tables together with enhanced HTML formatting\n",
    "display(HTML(html_null_values + html_missing_percentage + html_rows_with_missing_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What question do we want to ask of the data? \n",
    "\n",
    "\n",
    "1. Does Cabin Nature Impacts Survival Rate?  Investigate the trends  between the number of people in a cabin, the letter of the cabin and the survival rate. Investigate if we can use the cabin grouped by first letters as a categoricals variables. \n",
    "\n",
    "2. Does Ticket Nature Impacts Survival Rate? Invvestigate if there is relationship between the tickets number characters nature and the survival rate.\n",
    "\n",
    "3.  Is there a correlation between any numerical variables? the effects of dependable variables that i too correlated in data science "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Understanding the nature of cabins \n",
    "\n",
    "# Creating a new feature 'cabin_multiple'\n",
    "# This feature counts the number of cabins listed for each passenger (if any)\n",
    "training['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "\n",
    "# Analyze the distribution of cabin counts\n",
    "# This helps in understanding how many passengers have multiple cabin assignments\n",
    "cabin_count_distribution = training['cabin_multiple'].value_counts()\n",
    "\n",
    "# Display the distribution of cabin counts\n",
    "print(\"Distribution of Cabin Counts:\")\n",
    "print(cabin_count_distribution)\n",
    "\n",
    "# Create a pivot table to analyze the relationship between survival and the number of cabins\n",
    "# The table shows the count of tickets (or passengers) for each combination of survival status and cabin count\n",
    "\n",
    "pivot_table = pd.pivot_table(\n",
    "    training, \n",
    "    index='Survived',          # Rows will represent survival status\n",
    "    columns='cabin_multiple',  # Columns will represent the count of cabins\n",
    "    values='Ticket',           # Values in the table are counts of tickets\n",
    "    aggfunc='count'            # Aggregation function to count tickets\n",
    ")\n",
    "\n",
    "# Display the pivot table\n",
    "print(pivot_table)\n",
    "\n",
    "# Create a new feature 'cabin_adv' based on the first letter of the cabin\n",
    "# This process also treats null (NaN) values in 'Cabin' as a separate category\n",
    "training['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\n",
    "\n",
    "# Display the count of each category in the new 'cabin_adv' feature\n",
    "print(\"Cabin Advanced Feature Counts:\")\n",
    "print(training['cabin_adv'].value_counts())\n",
    "\n",
    "# Create a pivot table to compare survival rate by cabin category\n",
    "# The table shows the count of names (or passengers) for each combination of survival status and cabin category\n",
    "cabin_survival_pivot = pd.pivot_table(\n",
    "    training,\n",
    "    index='Survived',\n",
    "    columns='cabin_adv',\n",
    "    values='Name',\n",
    "    aggfunc='count'\n",
    ")\n",
    "\n",
    "# Display the pivot table\n",
    "print(\"\\nSurvival Rate by Cabin Category:\")\n",
    "print(cabin_survival_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Understanding Ticket values\n",
    "\n",
    "# Categorize tickets as numeric or non-numeric\n",
    "training['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "training['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n",
    "\n",
    "# Analyze the distribution of numeric vs non-numeric tickets\n",
    "print(\"Numeric Ticket Distribution:\")\n",
    "print(training['numeric_ticket'].value_counts())\n",
    "\n",
    "# Analyze the distribution of ticket letter categories\n",
    "print(\"\\nTicket Letter Category Distribution:\")\n",
    "print(training['ticket_letters'].value_counts())\n",
    "\n",
    "# Set pandas option to display all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Analyze survival rate by numeric vs non-numeric ticket type\n",
    "print(\"\\nSurvival Rate by Numeric vs Non-Numeric Ticket Type:\")\n",
    "numeric_ticket_pivot = pd.pivot_table(training, index='Survived', columns='numeric_ticket', values='Ticket', aggfunc='count')\n",
    "print(numeric_ticket_pivot)\n",
    "\n",
    "# Analyze survival rate across different ticket types (letters)\n",
    "print(\"\\nSurvival Rate by Ticket Letter Categories:\")\n",
    "ticket_letters_pivot = pd.pivot_table(training, index='Survived', columns='ticket_letters', values='Ticket', aggfunc='count')\n",
    "print(ticket_letters_pivot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyze the correlation between numerical variables \n",
    "\n",
    "# Print the correlation matrix\n",
    "corr_matrix = selected_numericals.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))  # Set the size of the heatmap\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True,       # Annotate cells with correlation values\n",
    "    cmap='coolwarm',  # Color map for different correlation values\n",
    "    fmt='.2f',        # Format of the annotations\n",
    "    linewidths=.5,    # Space between cells\n",
    "    cbar_kws={\"shrink\": .8}  # Adjust the color bar size\n",
    ")\n",
    "plt.title('Correlation Heatmap of Selected Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Steps for Modeling\n",
    "---\n",
    "**1. Handling Missing Values:**\n",
    "   - Remove the very few (only 2) missing values in the 'Embarked' column.\n",
    "\n",
    "**2. Feature Selection:**\n",
    "   - Focus on relevant variables for model efficiency, particularly due to limited data. Unnecessary features like 'Name' and 'Passenger ID' are excluded. The selected features are 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'.\n",
    "\n",
    "**3. Categorical Data Transformation:**\n",
    "   - Perform categorical transformations on all data. Typically, a transformer would be used, but manual transformation ensures consistent columns in both training and test sets. This method might also offer insights into the test data's structure. Note: For typical applications outside competitions, using a one-hot encoder is generally advised.\n",
    "\n",
    "**4. Imputation of Missing Data:**\n",
    "   - Impute missing 'Fare' and 'Age' data using the mean value. Experimentation with the median value is also a consideration.\n",
    "\n",
    "**5. Normalization of 'Fare':**\n",
    "   - Apply a logarithmic transformation to the 'Fare' feature to achieve a more normal-like distribution.\n",
    "\n",
    "**6. Scaling Features:**\n",
    "   - Scale the data between 0 and 1 using a standard scaler for uniformity across all features. \n",
    "\n",
    "These preprocessing steps are crucial for preparing the dataset, ensuring that it's clean, relevant, and formatted correctly for effective modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# -------------------\n",
    "\n",
    "# Adding a feature to count the number of cabins listed\n",
    "all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "\n",
    "# Extracting the first letter from the Cabin as a feature\n",
    "all_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\n",
    "\n",
    "# Determining if a ticket is numeric or not\n",
    "all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "\n",
    "# Extracting letters from the Ticket as a feature\n",
    "all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('/', '').lower() if len(x.split(' ')[:-1]) > 0 else 0)\n",
    "\n",
    "# Extracting titles from the Name as a feature\n",
    "all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "# Data Imputation\n",
    "# ---------------\n",
    "\n",
    "# Filling missing Age values with the median of the Age in the training set\n",
    "all_data.Age = all_data.Age.fillna(training.Age.median())\n",
    "\n",
    "# Filling missing Fare values with the median of the Fare in the training set\n",
    "all_data.Fare = all_data.Fare.fillna(training.Fare.median())\n",
    "\n",
    "# Dropping rows where 'Embarked' is missing\n",
    "all_data.dropna(subset=['Embarked'], inplace=True)\n",
    "\n",
    "# Data Normalization\n",
    "# ------------------\n",
    "\n",
    "# Applying log normalization to SibSp\n",
    "all_data['norm_sibsp'] = np.log(all_data.SibSp + 1)\n",
    "all_data['norm_sibsp'].hist()\n",
    "plt.title('Normalized SibSp')\n",
    "plt.show()\n",
    "\n",
    "# Applying log normalization to Fare\n",
    "all_data['norm_fare'] = np.log(all_data.Fare + 1)\n",
    "all_data['norm_fare'].hist()\n",
    "plt.title('Normalized Fare')\n",
    "plt.show()\n",
    "\n",
    "# Converting 'Pclass' to a categorical feature for one-hot encoding\n",
    "all_data.Pclass = all_data.Pclass.astype(str)\n",
    "\n",
    "# Creating Dummy Variables\n",
    "# ------------------------\n",
    "\n",
    "# One-hot encoding of categorical features\n",
    "all_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title', 'train_test']])\n",
    "\n",
    "# Splitting the Data\n",
    "# ------------------\n",
    "\n",
    "# Separating the training and test sets\n",
    "X_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis=1)\n",
    "X_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis=1)\n",
    "\n",
    "# Extracting the target variable 'Survived' for the training set\n",
    "y_train = all_data[all_data.train_test == 1].Survived\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data\n",
    "# ----------\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "# Create a copy of all_dummies to apply scaling\n",
    "all_dummies_scaled = all_dummies.copy()\n",
    "\n",
    "# Scale specific features: 'Age', 'SibSp', 'Parch', 'norm_fare'\n",
    "features_to_scale = ['Age', 'SibSp', 'Parch', 'norm_fare']\n",
    "all_dummies_scaled[features_to_scale] = scale.fit_transform(all_dummies_scaled[features_to_scale])\n",
    "\n",
    "# Splitting the Scaled Data\n",
    "# -------------------------\n",
    "\n",
    "# Separating the training and test sets from the scaled data\n",
    "X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis=1)\n",
    "X_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis=1)\n",
    "\n",
    "# Extracting the Target Variable\n",
    "# ------------------------------\n",
    "\n",
    "# Extracting the target variable 'Survived' for the training set\n",
    "y_train = all_data[all_data.train_test == 1].Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building: Establishing Baseline Performance\n",
    "\n",
    "To set a foundation for further analysis, it's important to assess the baseline performance of various models using their default settings. I utilized 5-fold cross-validation to gauge their initial effectiveness. This baseline serves as a benchmark to understand the impact of subsequent tuning on each model. However, it's crucial to note that high baseline performance on the validation set doesn't necessarily guarantee superior results on the final test set. This initial evaluation helps in identifying models with the most potential for improvements and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating Models\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "#  Baseline Model with Naive Bayes\n",
    "\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv = cross_val_score(gnb, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "# Output the cross-validation results\n",
    "print(\"Cross-validation scores:\", cv)\n",
    "print(\"Mean cross-validation score:\", cv.mean())\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "print(\"Logistic Regression - Cross-Validation Scores:\")\n",
    "cv_lr = cross_val_score(lr, X_train, y_train, cv=5)\n",
    "print(cv_lr)\n",
    "print(\"Average:\", cv_lr.mean())\n",
    "\n",
    "print(\"\\nLogistic Regression with Scaled Data:\")\n",
    "cv_lr_scaled = cross_val_score(lr, X_train_scaled, y_train, cv=5)\n",
    "print(cv_lr_scaled)\n",
    "print(\"Average:\", cv_lr_scaled.mean())\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt = tree.DecisionTreeClassifier(random_state=1)\n",
    "print(\"\\nDecision Tree Classifier - Cross-Validation Scores:\")\n",
    "cv_dt = cross_val_score(dt, X_train, y_train, cv=5)\n",
    "print(cv_dt)\n",
    "print(\"Average:\", cv_dt.mean())\n",
    "\n",
    "print(\"\\nDecision Tree with Scaled Data:\")\n",
    "cv_dt_scaled = cross_val_score(dt, X_train_scaled, y_train, cv=5)\n",
    "print(cv_dt_scaled)\n",
    "print(\"Average:\", cv_dt_scaled.mean())\n",
    "\n",
    "# K-Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "print(\"\\nK-Neighbors Classifier - Cross-Validation Scores:\")\n",
    "cv_knn = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "print(cv_knn)\n",
    "print(\"Average:\", cv_knn.mean())\n",
    "\n",
    "print(\"\\nK-Neighbors with Scaled Data:\")\n",
    "cv_knn_scaled = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "print(cv_knn_scaled)\n",
    "print(\"Average:\", cv_knn_scaled.mean())\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "print(\"\\nRandom Forest Classifier - Cross-Validation Scores:\")\n",
    "cv_rf = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(cv_rf)\n",
    "print(\"Average:\", cv_rf.mean())\n",
    "\n",
    "print(\"\\nRandom Forest with Scaled Data:\")\n",
    "cv_rf_scaled = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
    "print(cv_rf_scaled)\n",
    "print(\"Average:\", cv_rf_scaled.mean())\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc = SVC(probability=True)\n",
    "print(\"\\nSupport Vector Classifier with Scaled Data:\")\n",
    "cv_svc_scaled = cross_val_score(svc, X_train_scaled, y_train, cv=5)\n",
    "print(cv_svc_scaled)\n",
    "print(\"Average:\", cv_svc_scaled.mean())\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state=1)\n",
    "print(\"\\nXGBoost Classifier with Scaled Data:\")\n",
    "cv_xgb_scaled = cross_val_score(xgb, X_train_scaled, y_train, cv=5)\n",
    "print(cv_xgb_scaled)\n",
    "print(\"Average:\", cv_xgb_scaled.mean())\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('knn', knn), ('rf', rf), ('svc', svc), ('xgb', xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "print(\"\\nVoting Classifier with Scaled Data:\")\n",
    "cv_voting = cross_val_score(voting_clf, X_train_scaled, y_train, cv=5)\n",
    "print(cv_voting)\n",
    "print(\"Average:\", cv_voting.mean())\n",
    "\n",
    "# Training the Voting Classifier and Making Predictions\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "y_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\n",
    "\n",
    "# Creating Submission File\n",
    "basic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\n",
    "base_submission = pd.DataFrame(basic_submission)\n",
    "base_submission.to_csv('base_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created: 'base_submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
